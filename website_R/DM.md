## 数据挖掘笔记

### 1.决策树

Infogain(ID3)  Gainratio(C4.5, C5.0)  Gini(CART)

#### 1.1 Gini

倾向于选择多值变量，类别较多时分类困难。



### 2.组合预测模型

建立并综合参考多个预测模型，减少预测误差

1. 袋装技术：将多个不稳定的预测模型组成一个稳定模型
2. 随机森林
3. 推进技术：将多个稳定的弱模型结合成更有效的模型

评估指标：偏差和方差。**偏差大通常方差小，偏差小通常方差大**

#### 2.1 袋装技术

1. 建模阶段：样本集S，样本数量为n，从S中有放回随机抽取k次n个样本形成k个模型Ti
2. 预测阶段：为样本投票，少数服从多数。哪个类别“得票”最多，就预测为哪个类别。
3. 评价阶段：基于袋外观测（OOB）样本归入带外观测的概率约为1/e=0.368
   1. 依次输入所有样本
   2. 如果某样本在Ti中是OOB，则Ti可以为样本投票
   3. 少数服从多数
   4. 计算Ti的误差

k的决定：通常k=√p，p为变量个数。k越大，误差（偏差，各决策树的OOB预测误差）越小，方差（各决策树之间的相关性）越大

优点：降低方差以提高精确度；对【参数对样本数较为敏感（即“不稳定”）】的样本集作用显著；简单易懂
缺点：总有部分样本参与建模，结果偏乐观；模型不直观

> 实现袋装技术的R函数为ipred包中的bagging函数，它的基本书写格式为：
> `bagging(输出变量名~输入变量名,data=数据框名,nbagg=k,coob=TRUE,control=参数对象名)`
> 其中`coob=TRUE`表示基于袋外观测计算误差，`nbagg`用于指定袋装过程的自举次数。

#### 2.2 随机森林

1. 建模阶段：从S中随机抽取k阶样本形成Θi（Forest-RI），或随机抽取L个样本，生成[-1,1]均匀分布的L个随机数αi形成vi，vi=Σ αi · xi抽取k次产生k个线性组合样本形成Θi（Forest-RC,适用于小样本），生成的树充分生长，不剪技。
2. 预测阶段：为样本投票，少数服从多数
3. 评价阶段：基于OOB
   1. M个树，每个树计算预测误差ei
   2. 随机打乱第i个变量的顺序，再计算误差eij
   3. cij=ei-eij，cj=Σcij/M。cj表明第j个变量的重要性。

优点：运算效率高。仅依赖两个参数：k和样本数n，在不删除变量的条件下，可计算上千个样本，且效果好。

建立随机森林的R函数是randomForest包中的randomForest函数。
`randomForest(输出变量名~输入变量名,data=数据框名,mtry=k,ntre=M,importance =TRUE)`
`mtry`用于指定决策树各节点的输入变量个数k，`ntree`用于指定随机森林包包含M棵决策树，`importance = TRUE` 表示计算输入变量对输出变量重要性的测度值。

#### 2.3 推进技术

降低偏差， 通过样本权重和树的权重。

步骤与袋装技术类似，但

1. 第一次建模时，S中每个观测有相同的权重。在建模阶段抽祥时，各样本的权重较高越容易被抽中。

2. 每次建树后需修改样本权重。

   > 样本权重的修改：
   >
   > 正确样本：w(j,i+1)=w(j,i)\*(ei/(1-ei))。错误样本不变。
   >
   > 然后再归一化。

3. 投票时按树的权重进行。

   > 数的权重：log₂((1-ei)/ei) 或 1/2*log₂((1-ei)/ei) 

优点：无需过多参数，方法灵活，降低偏差。
缺点：不适用于过于复杂（过拟合）或过于弱（效果差）的模型

> 实现推进技术的R函数为adabag包中的boosting函数，它的基本书写格式为：
> `Boosting(输出变量名~输入变量名,data=数据框名,mfinal=重复次数,boos =TRUE, coeilearn=模型权重调整方法,control=参数对象名)`
> 其中`coob=TRUE`表示每次自举过程均调整各观测进入训练样本集的权重，`mfinal`用于指定袋装过程的自举次数。

### 3.K近邻

![KNN](C:/Users/zhouz/GitHub/SamZhou-2019.github.io/website_R/KNN.png)

#### “K”的决定

bigger K：分界平滑，噪声小， 忽略局部结构，方差小偏差大
smaller K：分界不平滑， 噪声大，体现局部结构，易过拟合，方差大偏差小

| 决定K的方法 | 留1法<br />1 VS Ohers，重复n次                               | 旁置法<br />train VS test                                    |
| ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 适用        | 小样本                                                       | 大样本                                                       |
| 原因        | 大样本成本高，<br />类别分布不均时效果差，<br />小样本学习充分 | 小样本学习充分性欠佳，<br />划分训练集和测试集的随机性误差会体现出来 |

#### 计算步骤

1. 数据标准化（xi’=(xi-min)/(max-min)或xi’=(xi-avg(x))/σ）
2. 选取未知分类的的点，计算相邻的点的距离
3. 选出距离最近的k个点
4. 根据k点的类标号决定未知点的分类

**1-近邻算法的预测错误概率不会高于普通贝叶斯预测方法错误率的2倍。**

#### 数据处理的方式

|        | 普通             | 加权                                                        |
| ------ | ---------------- | ----------------------------------------------------------- |
| 数值   | 原值或标准化     | xi’=xi/σ                                                    |
| 顺序型 | [0,1]均分取值    | 虚拟变量<br />(1, 1, …, 1)  (-1, 1, …, 1) … (-1, -1, …, -1) |
| 分类型 | 0：相同；1：不同 | 虚拟变量<br />(1, 0, …, 0) (0, 1, …, 0) … (0, 0, …, 1)      |

> R实现K－近邻法的函数是class包中的 knn函数。knn函数的基本书写格式为：
> `knn(train＝训练样本集,test＝测试样本集,cl=含出变量,k＝近邻个数K,prob=TRUE/FALSE,use.all = TRUE/FAISE)`
> 其中，TRUE表示函数返回值是概率值，FALSE表示类别值。回归预测中，prob应设置为 FALSE,分类预测中，prob可设置为TRUE; use. all TRUE表示当有多个等距离的近邻而使得实际近邻个数大于K时，所有近邻均参与预测 FAlSE 表示在多个等距离的近邻中随机抽取近邻，确保实际近邻个数等于K 。
> knn函数中的距离定义为欧氏距离。
> R中有关于1-近邻法的专用函数knnl，基本书写格式为：
> `Knn1(train=训练样本集,teast=测试样本集,cl=输出变量)`
> 此外，R中有将K-近邻法和留一法“打包”成一体的knn.cv，基本书写格式为：
> `Knn.cv(train=训练样本集,cl=输出变量,k=近邻个数K)`

#### 变量重要性加权

![KNNw](C:/Users/zhouz/GitHub/SamZhou-2019.github.io/website_R/KNNw.png)

其中e为剔除变量之后的预测误差，p为变量个数

#### 观测相似性加权

1. 数据处理（见上方“数据处理的方式”表格中“加权”一列）

2. 计算闵科夫斯基距离。顺序型，分类型在作差求k次方后还需除以虚拟变量个数
3. 确定k+1个近邻，并确定D(X(i),X(0))=d(X(i),X(0))/d(X(k+1),X(0))
4. 利用核函数求K[D(X(i),X(0))]即各观测权重Wi
5. 按分类将权重相加并比较

高斯核：K=(1/2π)e^(-d²/2)